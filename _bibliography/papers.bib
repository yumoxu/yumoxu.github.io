---
---
@inproceedings{xu2023oreo,
    title={Text Summarization with Oracle Expectation}, 
    author = "Xu, Yumo  and Lapata, Mirella",
    booktitle = "ICLR",
    year = "2023",
    address = "Online",
    abstract="Extractive summarization produces summaries by identifying and concatenating the most important sentences in a document. Since most summarization datasets do not come with gold labels indicating whether document sentences are summary-worthy, different labeling algorithms have been proposed to extrapolate oracle extracts for model training. In this work, we identify two flaws with the widely used greedy labeling approach: it delivers suboptimal and deterministic oracles. To alleviate both issues, we propose a simple yet effective labeling algorithm that creates soft, expectation-based sentence labels. We define a new learning objective for extractive summarization which incorporates learning signals from multiple oracle summaries and prove it is equivalent to estimating the oracle expectation for each document sentence. Without any architectural modifications, the proposed labeling scheme achieves superior performance on a variety of summarization benchmarks across domains and languages, in both supervised and zero-shot settings.",
    pdf = "https://openreview.net/pdf?id=HehQobsr0S",
    code = "https://github.com/yumoxu/oreo",
    selected={true},
    track={Summarization},
}

@article{xu2022neural,
      title={Document Summarization with Neural Query Modeling}, 
      author={Xu, Yumo},
      year={2022},
      abstract="Document summarization is a natural language processing task that aims to produce a short summary that concisely delivers the most important information of a document or multiple documents. Over the last few decades, the task has drawn much attention from both academia and industry, as it provides effective tools to manage and access text information. For example, through a newswire summarization engine, users can quickly digest a cluster of news articles by reading a short summary of the topic. Such summaries can, meanwhile, be used by news recommendation and question answering engines. Depending on the users’ role in the summarization process, document summarization falls into two broad categories: generic summarization and query focused summarization (QFS). The former focuses on information intrinsically salient in the input text, while the latter also caters to requests explicitly specified by users. Despite the difference between generic summarization and QFS in their task formulations, we argue that all summaries address queries, even if they are not formulated explicitly. In this thesis, we introduce query modeling in the document summarization context as a critical objective for incorporating observed or latent user intent. We investigate different approaches that explore this theme with deep neural networks. We develop novel systems with neural query modeling for both extractive summarization, where summaries are composed of salient segments (e.g., sentences) from the original document(s), and abstractive summarization, where summaries are made up of words or phrases that do not exist in the input. The recent availability of large-scale datasets has driven the development of neural models that create generic summaries. However, training data in the form of queries, documents, and summaries for QFS is scarce. As most existing research in QFS has employed an extractive approach, we first consider better modeling query-cluster interactions for low-resource extractive QFS. In contrast to previous work with retrieval-style methods for assembling query-relevant summaries, we propose a framework that progressively estimates whether text segments should be included in the summary. Notably, modules of this framework can be independently developed and can leverage training data if available. We present an instantiation of this framework with distant supervision from question answering where various resources exist to identify segments which are likely to answer the query. Experiments on benchmark datasets show that our framework achieves competitive results and is robust across domains. Ideally, summaries should be abstracts, and the hidden costs incurred by annotating QA pairs should be avoided in query modeling. The second part of this thesis focuses on the low-resource challenge in abstractive QFS, and builds an abstractive QFS system which is trained query-free. Concretely, we propose to decompose the task into query modeling and conditional language modeling. For query modeling, we first introduce a uniﬁed representation for summaries and queries to exploit training resources in generic summarization, on top of which a weakly supervised model is optimized for evidence estimation. The proposed framework achieves state-of-the-art performance in generating query focused abstracts across existing benchmarks. Finally, the third part of this thesis moves beyond QFS. We provide a uniﬁed modeling framework for any kind of summarization, under the assumption that all summaries are a response to a query, which is observed in the case of QFS and latent in the case of generic summarization. We model queries as discrete latent variables over document tokens, and learn representations compatible with observed and unobserved query verbalizations. Requiring no further optimization on downstream summarization tasks, experiments show that our approach outperforms strong comparison systems across benchmarks, query types, document settings, and target domains.",
      pdf = "https://era.ed.ac.uk/bitstream/handle/1842/39624/XuY_2022.pdf?sequence=1&isAllowed=y",
      selected={true},
      track={Summarization},
      publisher={The University of Edinburgh}
}


@article{xu2022doc,
      title={Document Summarization with Latent Queries}, 
      author={Xu, Yumo  and Lapata, Mirella},
      journal={TACL},
      year={2022},
      volume={abs/2106.00104},
      abstract="The availability of large-scale datasets has driven the development of neural models that create summaries from single documents, for generic purposes. When using a summarization system, users often have specific intents with various language realizations, which, depending on the information need, can range from a single keyword to a long narrative composed of multiple questions. Existing summarization systems, however, often either fail to support or act robustly on this query focused summarization task. We introduce LaQSum, the first unified text summarization system that learns Latent Queries from documents for abstractive summarization with any existing query forms. Under a deep generative framework, our system jointly optimizes a latent query model and a conditional language model, allowing users to plug-and-play queries of any type at test time. Despite learning from only generic summarization data and requiring no further optimization for downstream summarization tasks, our system robustly outperforms strong comparison systems across summarization benchmarks with different query types, document settings, and target domains.",
      pdf = "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00480/111219/Document-Summarization-with-Latent-Queries",
      code = "https://github.com/yumoxu/lqsum",
      selected={true},
      track={Summarization},
}


@inproceedings{Xu2020AbstractiveQF,
    title = "Generating Query Focused Summaries with Query-Free Resources",
    author = "Xu, Yumo  and
      Lapata, Mirella",
    booktitle = "ACL",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.475/",
    abstract="The availability of large-scale datasets has driven the development of neural sequence-to-sequence models to generate generic summaries, i.e., summaries which do not correspond to any pre-specified queries. However, due to the lack of training data, query focused summarization (QFS) has been studied mainly with extractive methods. In this work, we consider the problem of leveraging only generic summarization resources to build an abstractive QFS system. We propose Marge, a Masked ROUGE Regression framework composed of a novel unified representation for summaries and queries, and a distantly supervised training task for answer evidence estimation. To further utilize generic data for generation, three attributes are incorporated during training and inference to control the shape of the final summary: evidence rank, query guidance, and summary length. Despite learning from minimal supervision, our system achieves state-of-the-art results in the distantly supervised setting across domains and query types.",
    pdf = "https://aclanthology.org/2021.acl-long.475.pdf",
    code = "https://github.com/yumoxu/marge",
    selected={true},
    track={Summarization},
}

@article{Xu2020MetaDP,
  title={Meta Dialogue Policy Learning},
  author={Yumo Xu and Chenguang Zhu and Baolin Peng and Michael Zeng},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.02588},
  abstract = "Dialog policy determines the next-step actions for agents and hence is central to a dialogue system. However, when migrated to novel domains with little data, a policy model can fail to adapt due to insufficient interactions with the new environment. We propose Deep Transferable Q-Network (DTQN) to utilize shareable low-level signals between domains, such as dialogue acts and slots. We decompose the state and action representation space into feature subspaces corresponding to these low-level components to facilitate cross-domain knowledge transfer. Furthermore, we embed DTQN in a meta-learning framework and introduce Meta-DTQN with a dual-replay mechanism to enable effective off-policy training and adaptation. In experiments, our model outperforms baseline models in terms of both success rate and dialogue efficiency on the multi-domain dialogue dataset MultiWOZ 2.0.",
  pdf = "https://arxiv.org/pdf/2006.02588.pdf",
  track={Dialog},
}


@inproceedings{xu-lapata-2020-coarse,
    title = "Coarse-to-Fine Query Focused Multi-Document Summarization",
    author = "Xu, Yumo  and
      Lapata, Mirella",
    booktitle = "EMNLP",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.296",
    doi = "10.18653/v1/2020.emnlp-main.296",
    pages = "3632--3645",
    abstract = "We consider the problem of better modeling query-cluster interactions to facilitate query focused multi-document summarization. Due to the lack of training data, existing work relies heavily on retrieval-style methods for assembling query relevant summaries. We propose a coarse-to-fine modeling framework which employs progressively more accurate modules for estimating whether text segments are relevant, likely to contain an answer, and central. The modules can be independently developed and leverage training data if available. We present an instantiation of this framework with a trained evidence estimator which relies on distant supervision from question answering (where various resources exist) to identify segments which are likely to answer the query and should be included in the summary. Our framework is robust across domains and query types (i.e., long vs short) and outperforms strong comparison systems on benchmark datasets.",
    pdf = "https://www.aclweb.org/anthology/2020.emnlp-main.296.pdf",
    code = "https://github.com/yumoxu/querysum",
    selected={true},
    track={Summarization},
}

@article{xu-lapata-2019-weakly,
    title = "Weakly Supervised Domain Detection",
    author = "Xu, Yumo  and
      Lapata, Mirella",
    journal = "TACL",
    volume = "7",
    month = mar,
    year = "2019",
    url = "https://www.aclweb.org/anthology/Q19-1037",
    doi = "10.1162/tacl_a_00287",
    pages = "581--596",
    abstract = "In this paper we introduce domain detection as a new natural language processing task. We argue that the ability to detect textual segments that are domain-heavy (i.e., sentences or phrases that are representative of and provide evidence for a given domain) could enhance the robustness and portability of various text classification applications. We propose an encoder-detector framework for domain detection and bootstrap classifiers with multiple instance learning. The model is hierarchically organized and suited to multilabel classification. We demonstrate that despite learning with minimal supervision, our model can be applied to text spans of different granularities, languages, and genres. We also showcase the potential of domain detection for text summarization.",
    pdf = "https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00287",
    data = "https://drive.google.com/drive/u/1/folders/1K5TdwoezGzzb19_2QjTuNipOX9kf1tUY?usp=sharing",
    code =  "https://github.com/yumoxu/detnet",
    track={Summarization},
}

@inproceedings{sherborne-etal-2020-bootstrapping,
    title = "Bootstrapping a Crosslingual Semantic Parser",
    author = "Sherborne, Tom  and
      Xu, Yumo  and
      Lapata, Mirella",
    booktitle = "Findings of EMNLP",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.findings-emnlp.45",
    doi = "10.18653/v1/2020.findings-emnlp.45",
    pages = "499--517",
    abstract = "Recent progress in semantic parsing scarcely considers languages other than English but professional translation can be prohibitively expensive. We adapt a semantic parser trained on a single language, such as English, to new languages and multiple domains with minimal annotation. We query if machine translation is an adequate substitute for training data, and extend this to investigate bootstrapping using joint training with English, paraphrasing, and multilingual pre-trained models. We develop a Transformer-based parser combining paraphrases by ensembling attention over multiple encoders and present new versions of ATIS and Overnight in German and Chinese for evaluation. Experimental results indicate that MT can approximate training data in a new language for accurate parsing when augmented with paraphrasing through multiple MT engines. Considering when MT is inadequate, we also find that using our approach achieves parsing accuracy within 2{\%} of complete translation using only 50{\%} of training data.",
    pdf = "https://arxiv.org/pdf/2004.02585.pdf",
    code = "https://github.com/tomsherborne/bootstrap",
    track={Dialog},
}

@inproceedings{Zhang2019TrainableDS,
  title={Trainable Dynamic Subsampling for End-to-End Speech Recognition},
  author={Shucong Zhang and Erfan Loweimi and Yumo Xu and Peter Bell and Steve Renals},
  booktitle={Interspeech},
  year={2019},
  abstract="Jointly optimised attention-based encoder-decoder models have yielded impressive speech recognition results. The recurrent neural network (RNN) encoder is a key component in such models – it learns the hidden representations of the inputs. However, it is difficult for RNNs to model the long sequences characteristic of speech recognition. To address this,
            subsampling between stacked recurrent layers of the encoder is
            commonly employed. This method reduces the length of the
            input sequence and leads to gains in accuracy. However, static
            subsampling may both include redundant information and miss
            relevant information.
            We propose using a dynamic subsampling RNN (dsRNN) encoder. Unlike a statically subsampled RNN encoder, the dsRNN encoder can learn to skip redundant frames. Furthermore, the skip ratio may vary at different stages of training,
            thus allowing the encoder to learn the most relevant information for each epoch. Although the dsRNN is unidirectional, it
            yields lower phone error rates (PERs) than a bidirectional RNN
            on TIMIT. The dsRNN encoder has a 16.8% PER on the TIMIT
            test set, a considerable improvement over static subsampling
            methods used with unidirectional and bidirectional RNN encoders (23.5% and 20.4% PER respectively).",
  pdf = "https://www.isca-speech.org/archive/Interspeech_2019/pdfs/2778.pdf",
  code = "https://github.com/qishuxiyou/dsRNN",
  track={Dialog},
}

@inproceedings{xu-cohen-2018-stock,
    title = "Stock Movement Prediction from Tweets and Historical Prices",
    author = "Xu, Yumo  and
      Cohen, Shay B.",
    booktitle = "ACL",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1183",
    doi = "10.18653/v1/P18-1183",
    pages = "1970--1979",
    abstract = "Stock movement prediction is a challenging problem: the market is highly stochastic, and we make temporally-dependent predictions from chaotic data. We treat these three complexities and present a novel deep generative model jointly exploiting text and price signals for this task. Unlike the case with discriminative or topic modeling, our model introduces recurrent, continuous latent variables for a better treatment of stochasticity, and uses neural variational inference to address the intractable posterior inference. We also provide a hybrid objective with temporal auxiliary to flexibly capture predictive dependencies. We demonstrate the state-of-the-art performance of our proposed model on a new stock movement prediction dataset which we collected.",
    pdf = "http://aclweb.org/anthology/P18-1183",
    data = "https://github.com/yumoxu/stocknet-dataset",
    track={Others},
}